# Machine Learning Project
This is the dataset that I have used: https://www.kaggle.com/datasets/himanshunakrani/iris-dataset
You can read the project instructions that I followed below:
Στόχος της εργασίας είναι η υλοποίηση και εφαρμογή μεθόδων που παρουσιάστηκαν
στις διαλέξεις και στα φροντιστήρια σε ένα classification task της επιλογής σας. Το
dataset που θα διαλέξετε να χρησιμοποιήσετε θα πρέπει να περιλαμβάνει τουλάχιστον
3 ξεχωριστές κλάσεις (multiclass classification). Το ίδιο θα πρέπει να ισχύει και για το
πλήθος των features. Από τα datasets που μπορείτε να χρησιμοποιήστε αποκλείονται
τα MNIST datasets (π.χ. handwritten digits, fashion MNIST κλπ.), μερικα από τα οποία
έχουμε χρησιμοποιήσει και στα φροντιστήρια.
Πιο συγκεκριμένα καλείστε να υλοποιήστε τις εξής μεθόδους :

1. Τον αλγόριθμο Principal Component Analysis (PCA), για την μείωση των
διαστάσεων των δεδομένων σας. Αν n είναι το πλήθος των features των
δεδομένων σας θα πρέπει να εκτελέσετε τον αλγόριθμο για ένα m ε [2, n) της
επιλογής σας. Αρκεί η επίδειξη των αποτελεσμάτων στα δεδομένα, ενώ η
εφαρμογή του στις υπόλοιπες μεθόδους δεν είναι απαραίτητη.

2. Τον αλγόριθμο ελαχίστων τετραγώνων (least squares). Στα πλαίσια των
φροντιστηρίων αναπτύξαμε τον αλγόριθμο σε ένα regression task. Καλείστε να
προσαρμόσετε τον αλγόριθμο στο classification task που επιλέξατε.

3. Τον αλγόριθμο λογιστικής παλινδρόμησης (logistic regression). Θα πρέπει να
εκπαιδεύσετε το μοντέλο σας με τη χρήση του αλγορίθμου Stochastic Gradient
Descent. Ως loss function θα πρέπει να χρησιμοποιήσετε το Cross Entropy Loss.

4. Τον αλγόριθμο Κ κοντινότερων γειτόνων (K Nearest Neighbors). Θα πρέπει να
βρείτε τη βέλτιστη τιμή της υπερπαραμέτρου Κ στο διάστημα [1, 10].
Παρουσιάστε τα αποτελέσματα σας για κάθε τιμή της Κ.

5. Τον αλγόριθμο Naïve Bayes. ΠΡΟΣΟΧΗ! Η μετατροπή των features σε binary
δεδομένα (τιμές 0 ή 1), όπως παρουσιάστηκαν στα φροντιστήρια δεν θα γίνει
αποδεκτή. Αντίθετα, μπορείτε να εφαρμόσετε κανονικές κατανομές με
διαγώνιους πίνακες συμμεταβλητότητας (όχι κατ’ ανάγκη κοινούς για κάθε
κλάση), που είναι ο Naïve Bayes για δεδομένα στο R^d.

6. Έναν Multilayer Perceptron (νευρωνικό δίκτυο με πολλαπλά γραμμικά επίπεδα
σε συνδυασμό με μη γραμμικές συναρτήσεις ενεργοποίησης) μέσω του Pytorch
framework. Είστε ελεύθεροι να επιλέξετε το πλήθος των επιπέδων, τον τύπο
των συναρτήσεων ενεργοποίησης, το learning rate καθώς επίσης και
οποιοδήποτε άλλη υπερπαράμετρο. Ισχύουν οι ίδιοι περιορισμοί με τη
λογιστική παλινδρόμηση.

7. Τον αλγόριθμο K-Means. Ο αλγόριθμος αφορά την περίπτωση συσταδοποίησης
(clustering), οπότε, μόνο γι’ αυτή τη περίπτωση, θεωρείστε ότι η κλάση του κάθε
παραδείγματος των δεδομένων σας είναι άγνωστος. Εκτελέστε τον αλγόριθμο
για έναν αριθμό συστάδων ίσο με τον πλήθος των διαφορετικών κλάσεων του
dataset που επιλέξατε.
